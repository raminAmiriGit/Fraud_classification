{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "34a6753b-bc31-4d27-a889-0331f5752f83",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Mistplay Fraud Demo â€“ Train & Register Model\n",
    "\n",
    "This notebook uses Feature Store to build a training set, trains a fraud classifier, tracks metrics with MLflow, and registers the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "22c4ebf2-d3b1-4586-8382-99fdd489d41d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# !pip install mlflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea9b4046-baf7-4326-aa80-52d10cc7cc9f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install databricks-feature-engineering>=0.13.0a4 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "53820608-c9ac-4700-bf09-55b9b4356c9b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install \"pyarrow>=16,<21\"\n",
    "dbutils.library.restartPython()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f7d4cfb-a0b9-49f6-b1ff-90ebc5fa572d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "!pip list | grep pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b34a177b-3a8d-496a-8a33-031fe965cf43",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 5"
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import mlflow.data\n",
    "import mlflow.sklearn\n",
    "import pandas as pd\n",
    "from databricks.feature_store import FeatureStoreClient, FeatureLookup\n",
    "from pyspark.sql import functions as F\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import average_precision_score, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "\n",
    "DB_NAME = \"ramin_serverless_aws_catalog.mistplay_fraud_demo\"\n",
    "MODEL_LR = \"ramin_serverless_aws_catalog.mistplay_fraud_demo.mistplay_fraud_model_lr\"\n",
    "MODEL_RF = \"ramin_serverless_aws_catalog.mistplay_fraud_demo.mistplay_fraud_model_rf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a99fee2e-99c7-4e7f-abfc-938f1e296fb4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "fs = FeatureStoreClient()\n",
    "\n",
    "training_base = spark.table(f\"{DB_NAME}.training_base\")\n",
    "\n",
    "# lookups = [\n",
    "#     FeatureLookup(table_name=f\"{DB_NAME}.account_features\", lookup_key=\"account_id\"),\n",
    "#     FeatureLookup(table_name=f\"{DB_NAME}.device_features\", lookup_key=\"device_id\"),\n",
    "# ]\n",
    "\n",
    "# training_set = fs.create_training_set(\n",
    "#     training_base,\n",
    "#     feature_lookups=lookups,\n",
    "#     label=\"is_fraud_label\",\n",
    "#     exclude_columns=[\"account_id\", \"device_id\"],\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a39fb66d-c445-40eb-ae44-e11ec8b76574",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# When creating a training set, you can combine:\n",
    "# 1. Regular feature lookups (pre-computed features)\n",
    "# 2. On-demand features (computed at inference time)\n",
    "\n",
    "from databricks.feature_engineering import FeatureLookup\n",
    "\n",
    "training_set = fs.create_training_set(\n",
    "    df=spark.table(f\"{DB_NAME}.training_base\"),\n",
    "    feature_lookups=[\n",
    "        FeatureLookup(\n",
    "            table_name=f\"{DB_NAME}.account_features\",\n",
    "            lookup_key=\"account_id\",\n",
    "            feature_names=[\n",
    "                \"country\",\n",
    "                \"platform\",\n",
    "                \"marketing_channel\",\n",
    "                \"events_7d\",\n",
    "                \"avg_session_minutes_7d\",\n",
    "                \"vpn_rate_7d\",\n",
    "                \"distinct_devices_7d\",\n",
    "                \"rewards_7d\",\n",
    "                \"reward_amount_7d\",\n",
    "                \"reward_amount_since_last_batch\"\n",
    "            ]\n",
    "        ),\n",
    "        FeatureLookup(\n",
    "            table_name=f\"{DB_NAME}.device_features\",\n",
    "            lookup_key=\"device_id\",\n",
    "            feature_names=[\n",
    "                \"device_type\",\n",
    "                \"os_version\",\n",
    "                \"is_emulator\",\n",
    "                \"device_risk_score\",\n",
    "                \"accounts_per_device_7d\"\n",
    "            ]\n",
    "        )\n",
    "    ],\n",
    "    feature_function=f\"{DB_NAME}.last_batch_cutoff\",  # Add on-demand features here\n",
    "    label=\"is_fraud_label\",\n",
    "    exclude_columns=[\"account_id\", \"device_id\"]\n",
    ")\n",
    "\n",
    "# Load the training data\n",
    "training_df = training_set.load_df()\n",
    "display(training_df.limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7a88a7ca-74dc-469e-a791-d57f31e1b649",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## scikitlearn package to use (normal ML)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6b398bdb-3299-48fc-8e8e-1f86e1db9fbb",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 8"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "training_df = training_set.load_df().toPandas()\n",
    "\n",
    "label_col = \"is_fraud_label\"\n",
    "categorical_cols = [\n",
    "    \"country\",\n",
    "    \"platform\",\n",
    "    \"marketing_channel\",\n",
    "    \"device_type\",\n",
    "    \"os_version\",\n",
    "]\n",
    "\n",
    "numeric_cols = [\n",
    "    c for c in training_df.columns if c not in categorical_cols + [label_col, \"last_batch_cutoff\"]\n",
    "]\n",
    "\n",
    "X = training_df.drop(columns=[label_col])\n",
    "y = training_df[label_col]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"categorical\", OneHotEncoder(handle_unknown=\"ignore\"), categorical_cols),\n",
    "        (\"numeric\", StandardScaler(), numeric_cols),\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "80e9c2b9-0b6f-4036-a204-22811da9f013",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "training_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d0f8ab5b-ac0b-4453-9cec-fd72b314e3fa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## mlflow to track metrics and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a9bb1de3-1af3-485a-9ee4-8c882972d363",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 11"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def train_and_log(model_name: str, estimator):\n",
    "    pipeline = Pipeline(steps=[(\"preprocess\", preprocess), (\"model\", estimator)])\n",
    "    \n",
    "    with mlflow.start_run(run_name=model_name) as run:\n",
    "        # Create dataset objects for train and test splits\n",
    "        train_data = pd.concat([X_train, y_train.reset_index(drop=True)], axis=1)\n",
    "        test_data = pd.concat([X_test, y_test.reset_index(drop=True)], axis=1)\n",
    "        \n",
    "        train_dataset = mlflow.data.from_pandas(\n",
    "            train_data,\n",
    "            source=f\"{DB_NAME}.training_base\",\n",
    "            name=\"fraud-detection-train\",\n",
    "            targets=label_col\n",
    "        )\n",
    "        \n",
    "        test_dataset = mlflow.data.from_pandas(\n",
    "            test_data,\n",
    "            source=f\"{DB_NAME}.training_base\",\n",
    "            name=\"fraud-detection-test\",\n",
    "            targets=label_col\n",
    "        )\n",
    "        \n",
    "        # Log datasets to MLflow\n",
    "        mlflow.log_input(train_dataset, context=\"training\")\n",
    "        mlflow.log_input(test_dataset, context=\"testing\")\n",
    "        \n",
    "        # Train and evaluate\n",
    "        pipeline.fit(X_train, y_train)\n",
    "        preds = pipeline.predict_proba(X_test)[:, 1]\n",
    "\n",
    "        roc_auc = roc_auc_score(y_test, preds)\n",
    "        pr_auc = average_precision_score(y_test, preds)\n",
    "\n",
    "        mlflow.log_metric(\"roc_auc\", roc_auc)\n",
    "        mlflow.log_metric(\"pr_auc\", pr_auc)\n",
    "\n",
    "        fs.log_model(\n",
    "            model=pipeline,\n",
    "            artifact_path=\"model\",\n",
    "            flavor=mlflow.sklearn,\n",
    "            training_set=training_set,\n",
    "            registered_model_name=model_name,\n",
    "            extra_pip_requirements=[\"pyarrow>=16,<21\"]\n",
    "        )\n",
    "\n",
    "    return {\n",
    "        \"model_name\": model_name,\n",
    "        \"roc_auc\": roc_auc,\n",
    "        \"pr_auc\": pr_auc,\n",
    "    }\n",
    "\n",
    "results = []\n",
    "\n",
    "results.append(\n",
    "    train_and_log(\n",
    "        MODEL_LR,\n",
    "        LogisticRegression(max_iter=200, class_weight=\"balanced\"),\n",
    "    )\n",
    ")\n",
    "\n",
    "results.append(\n",
    "    train_and_log(\n",
    "        MODEL_RF,\n",
    "        RandomForestClassifier(\n",
    "            n_estimators=200,\n",
    "            max_depth=8,\n",
    "            random_state=42,\n",
    "            class_weight=\"balanced\",\n",
    "        ),\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3abf1831-ae4e-4547-991c-9bea9a8f5bd6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Create another version of model using less amount of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9207b67f-20c1-4e3e-8057-836cb03a3fa1",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 13"
    }
   },
   "outputs": [],
   "source": [
    "X_sampled = training_df.drop(columns=[label_col])\n",
    "y_sampled = training_df[label_col]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_sampled, y_sampled, test_size=0.99, random_state=42, stratify=y_sampled\n",
    ")\n",
    "\n",
    "def train_and_log(model_name: str, estimator):\n",
    "    pipeline = Pipeline(steps=[(\"preprocess\", preprocess), (\"model\", estimator)])\n",
    "    \n",
    "    with mlflow.start_run(run_name=model_name) as run:\n",
    "        # Create dataset objects for train and test splits\n",
    "        train_data = pd.concat([X_train, y_train.reset_index(drop=True)], axis=1)\n",
    "        test_data = pd.concat([X_test, y_test.reset_index(drop=True)], axis=1)\n",
    "        \n",
    "        train_dataset = mlflow.data.from_pandas(\n",
    "            train_data,\n",
    "            source=f\"{DB_NAME}.training_base\",\n",
    "            name=\"fraud-detection-train-sampled\",\n",
    "            targets=label_col\n",
    "        )\n",
    "        \n",
    "        test_dataset = mlflow.data.from_pandas(\n",
    "            test_data,\n",
    "            source=f\"{DB_NAME}.training_base\",\n",
    "            name=\"fraud-detection-test-sampled\",\n",
    "            targets=label_col\n",
    "        )\n",
    "        \n",
    "        # Log datasets to MLflow\n",
    "        mlflow.log_input(train_dataset, context=\"training\")\n",
    "        mlflow.log_input(test_dataset, context=\"testing\")\n",
    "        \n",
    "        # Train and evaluate\n",
    "        pipeline.fit(X_train, y_train)\n",
    "        preds = pipeline.predict_proba(X_test)[:, 1]\n",
    "\n",
    "        roc_auc = roc_auc_score(y_test, preds)\n",
    "        pr_auc = average_precision_score(y_test, preds)\n",
    "\n",
    "        mlflow.log_metric(\"roc_auc\", roc_auc)\n",
    "        mlflow.log_metric(\"pr_auc\", pr_auc)\n",
    "\n",
    "        fs.log_model(\n",
    "            model=pipeline,\n",
    "            artifact_path=\"model\",\n",
    "            flavor=mlflow.sklearn,\n",
    "            training_set=training_set,\n",
    "            registered_model_name=model_name,\n",
    "            extra_pip_requirements=[\"pyarrow>=16,<21\"]\n",
    "        )\n",
    "\n",
    "    return {\n",
    "        \"model_name\": model_name,\n",
    "        \"roc_auc\": roc_auc,\n",
    "        \"pr_auc\": pr_auc,\n",
    "    }\n",
    "\n",
    "results = []\n",
    "\n",
    "results.append(\n",
    "    train_and_log(\n",
    "        MODEL_LR,\n",
    "        LogisticRegression(max_iter=200, class_weight=\"balanced\"),\n",
    "    )\n",
    ")\n",
    "\n",
    "results.append(\n",
    "    train_and_log(\n",
    "        MODEL_RF,\n",
    "        RandomForestClassifier(\n",
    "            n_estimators=200,\n",
    "            max_depth=8,\n",
    "            random_state=42,\n",
    "            class_weight=\"balanced\",\n",
    "        ),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fc782c6c-00c0-46ed-844a-1d4d58d76f11",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "398bc004-bd78-4377-a6ae-ce0c310fd113",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Online Inference\n",
    "\n",
    "### First the feature tables must be synced to an online table\n",
    "\n",
    "create online tables through UI or following api. see https://docs.databricks.com/en/machine-learning/feature-store/online-tables.html#create-an-online-table-using-apis\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5a8582c2-3d0e-411f-9524-800ab0f1e920",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Use api to create online tables, Use below once to create the online table\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "09ea16b5-2ecb-4193-b9dc-0483b3c1aaf9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks.feature_engineering import FeatureEngineeringClient\n",
    "\n",
    "# Initialize the client\n",
    "fe = FeatureEngineeringClient()\n",
    "\n",
    "# Create an online store with specified capacity\n",
    "fe.create_online_store(\n",
    "    name=\"ramin-online-store-2\",\n",
    "    capacity=\"CU_1\"  # Valid options: \"CU_1\", \"CU_2\", \"CU_4\", \"CU_8\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "860727bc-a2f5-4fb2-82ac-2846e30e5dae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Cell directly below uses feature engineering client. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "021b23a8-6c4e-4144-8042-44df630ee090",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# from databricks.feature_engineering import FeatureEngineeringClient\n",
    "\n",
    "# fe = FeatureEngineeringClient()\n",
    "\n",
    "# # OPTION A: List what exists\n",
    "# stores = fe.list_online_stores()\n",
    "# for s in stores:\n",
    "#     print(s.name, s.state, s.capacity)\n",
    "\n",
    "# # OPTION B: Get (and check) a specific store\n",
    "# online_store = fe.get_online_store(name=\"my-online-store\")\n",
    "# print(online_store)  # should NOT be None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a135a9e9-a421-4e2d-8e95-312ebf99e3b2",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 20"
    }
   },
   "outputs": [],
   "source": [
    "from databricks.ml_features.entities.online_store import DatabricksOnlineStore\n",
    "from databricks.feature_engineering import FeatureEngineeringClient\n",
    "\n",
    "# Initialize the client\n",
    "fe = FeatureEngineeringClient()\n",
    "# Get the online store instance\n",
    "online_store = fe.get_online_store(name=\"ramin-online-store-2\")\n",
    "\n",
    "# Publish the feature table to the online store\n",
    "fe.publish_table(\n",
    "    online_store=online_store,\n",
    "    source_table_name=\"ramin_serverless_aws_catalog.mistplay_fraud_demo.account_features\",\n",
    "    online_table_name = \"ramin_serverless_aws_catalog.mistplay_fraud_demo.account_features-online\",\n",
    "    publish_mode=\"SNAPSHOT\"\n",
    ")\n",
    "\n",
    "fe.publish_table(\n",
    "    online_store=online_store,\n",
    "    source_table_name=\"ramin_serverless_aws_catalog.mistplay_fraud_demo.device_features\",\n",
    "    online_table_name = \"ramin_serverless_aws_catalog.mistplay_fraud_demo.device_features-online\",\n",
    "    publish_mode=\"SNAPSHOT\"\n",
    ")\n",
    "# fe.publish_table(\n",
    "#     online_store=online_store,\n",
    "#     source_table_name=\"ramin_serverless_aws_catalog.mistplay_fraud_demo.transaction_aggregates\",\n",
    "#     online_table_name = \"ramin_serverless_aws_catalog.mistplay_fraud_demo.transaction_aggregates-online\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2b3a1111-a240-4975-b122-1f1480607b15",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "best_row = results_df.sort_values(\"roc_auc\", ascending=False).iloc[0]\n",
    "\n",
    "best_model_name = best_row[\"model_name\"]\n",
    "best_roc_auc = float(best_row[\"roc_auc\"])\n",
    "\n",
    "best_model_df = spark.createDataFrame([\n",
    "    {\n",
    "        \"model_name\": best_model_name,\n",
    "        \"selection_metric\": \"roc_auc\",\n",
    "        \"metric_value\": best_roc_auc,\n",
    "        \"selected_at\": None,\n",
    "    }\n",
    "]).withColumn(\"selected_at\", F.current_timestamp())\n",
    "\n",
    "best_model_df.write.mode(\"overwrite\").saveAsTable(f\"{DB_NAME}.model_selection\")\n",
    "\n",
    "print(\"Registered models:\")\n",
    "print(f\"- {MODEL_LR}\")\n",
    "print(f\"- {MODEL_RF}\")\n",
    "print(\"Best model by ROC AUC:\")\n",
    "print(best_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bd6e0528-40b4-431f-a20e-e3d54d17fee6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "02_train_register",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
